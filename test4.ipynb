{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.animate_ball3 import BreakoutGame\n",
    "%matplotlib tk\n",
    "game = BreakoutGame(framesize=(64,64), xy=[[2,2],[5,2],[5,2],[5,2]], color=255, direction=[[1,1],[1,1],[1,2],[1,3]],nr_sim_steps=10,obstacle_color=200,destructible_obstacles=True,boundary_color=50,player_color=255,nr_balls=4)\n",
    "\n",
    "for i in range(20):\n",
    "    game.gs.add_obstacles([[i+10,29]])\n",
    "game.main_game_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from utils.dqn_agent import DQN as DQNAgent\n",
    "from utils.animate_ball3 import BreakoutGame\n",
    "import random\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"runs/\"+str(datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "# We create our gym environment \n",
    "#env = BreakoutGame(framesize=(64,64), xy=[[5,2]], color=255, direction=[[1,1]],nr_sim_steps=10,obstacle_color=200,destructible_obstacles=False,boundary_color=50,player_color=255,nr_balls=1)\n",
    "# We get the shape of a state and the actions space size\n",
    "state_size = 1*2*2+1\n",
    "action_size = 3#left,right,none\n",
    "# Number of episodes to run\n",
    "n_episodes = 15000\n",
    "\n",
    "# Max iterations per epiode\n",
    "max_iteration_ep = 500\n",
    "# We define our agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "agent.lr=1e-6\n",
    "agent.gamma=0.99\n",
    "agent.exploration_proba=1\n",
    "agent.exploration_proba_decay=0.001\n",
    "agent.batch_size=32\n",
    "agent.verbose=True\n",
    "batch_size=agent.batch_size\n",
    "total_steps = 0\n",
    "\n",
    "# We iterate over episodes\n",
    "for e in range(n_episodes):\n",
    "    # We initialize the first state and reshape it to fit \n",
    "    #  with the input layer of the DNN\n",
    "    dirx = random.random()*8-4\n",
    "    diry = random.random()*8-4#\n",
    "    dirx=random.choice([-2,-1,0,1,2])\n",
    "    diry=random.choice([-2,-1,1,2])\n",
    "    xy=[random.randint(2,28),random.randint(10,28)]\n",
    "\n",
    "    env = BreakoutGame(framesize=(32,32), xy=[xy], color=255, direction=[[dirx,diry]],nr_sim_steps=10,obstacle_color=200,destructible_obstacles=False,boundary_color=50,player_color=255,nr_balls=1)\n",
    "    current_state = env.gs.get_state()\n",
    "    current_state = np.array([current_state])\n",
    "\n",
    "\n",
    "    for step in range(max_iteration_ep):\n",
    "        \n",
    "        total_steps = total_steps + 1\n",
    "        # the agent computes the action to perform\n",
    "        action = agent.compute_action(current_state)\n",
    "        if action==0:\n",
    "            action=[0,0]\n",
    "        if action==1:\n",
    "            action=[2,0]\n",
    "        if action==2:\n",
    "            action=[-2,0]\n",
    "        # the envrionment runs the action and returns\n",
    "        # the next state, a reward and whether the agent is done\n",
    "\n",
    "        ##next_state, reward, done, _,_ = env.step(action)\n",
    "        next_state, reward, done = env.rl_refresh_frame(action)\n",
    "        #print(current_state,step,reward)\n",
    "        next_state = np.array([next_state])\n",
    "        #print(next_state)\n",
    "        \n",
    "        # We sotre each experience in the memory buffer\n",
    "        #print(\"current_state\",current_state,\"action\",action,\"reward\",reward,\"next_state\",next_state,\"done\",done)\n",
    "        agent.store_episode(current_state, action, reward, next_state, done)\n",
    "        \n",
    "        # if the episode is ended, we leave the loop after\n",
    "        # updating the exploration probability\n",
    "        if done:\n",
    "            agent.update_exploration_probability()\n",
    "            break\n",
    "        current_state = next_state\n",
    "    # if the have at least batch_size experiences in the memory buffer\n",
    "    # than we tain our model\n",
    "    if total_steps >= batch_size:\n",
    "        loss=agent.train()\n",
    "        try:\n",
    "            writer.add_scalar(\"Loss/train\", loss, e)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "plt.ion()\n",
    "fig = plt.figure(1)\n",
    "env = BreakoutGame(framesize=(32,32), xy=[[20,20]], color=255, direction=[[3,1]],nr_sim_steps=10,obstacle_color=200,destructible_obstacles=False,boundary_color=50,player_color=255,nr_balls=1)\n",
    "\n",
    "\n",
    "while env.has_ended==False:\n",
    "    current_state = env.gs.get_state()\n",
    "    current_state = np.array([current_state])\n",
    "    action = agent.compute_action(current_state)\n",
    "    if action==0:\n",
    "        action=[0,0]\n",
    "    if action==1:\n",
    "        action=[2,0]\n",
    "    if action==2:\n",
    "        action=[-2,0]\n",
    "    env.gs.player_direction=action\n",
    "    frame=env.refresh_frame()\n",
    "            #frameT = frame.transpose((1,0))\n",
    "            #plt.clf()\n",
    "    plt.imshow(np.flip(frame.T,0))\n",
    "    plt.pause(0.03)\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
